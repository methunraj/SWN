# Swift Neethi

A complete AI chat application with a React frontend and Python FastAPI backend, supporting local LLM providers like Ollama and llama.cpp.

![Swift Neethi Interface](https://img.shields.io/badge/Interface-ChatGPT--like-green)
![Backend](https://img.shields.io/badge/Backend-FastAPI-blue)
![Frontend](https://img.shields.io/badge/Frontend-React-blue)
![AI Providers](https://img.shields.io/badge/AI-Ollama%20%7C%20llama.cpp-orange)

## ğŸŒŸ Features

### Frontend (React TypeScript)
- ğŸ¨ **ChatGPT-like Interface**: Beautiful dark theme with sidebar and chat area
- ğŸ’¬ **Real-time Streaming**: Messages stream in real-time from AI
- ğŸ“± **Responsive Design**: Works perfectly on desktop and mobile
- ğŸ’¾ **Chat History**: Multiple conversations with automatic saving
- âš¡ **Fast & Lightweight**: Built with modern React and TypeScript

### Backend (Python FastAPI)
- ğŸ¤– **Multi-Provider Support**: Seamlessly switch between Ollama and llama.cpp
- ğŸ§  **Agent Architecture**: Modular agents for chat, memory, and prompt management
- ğŸ“¡ **Streaming API**: Real-time responses via Server-Sent Events
- ğŸ¯ **System Prompts**: Dynamic prompt templates with variable substitution
- ğŸ”’ **Rate Limiting**: Built-in protection and error handling
- ğŸ“Š **REST API**: Complete API with automatic documentation

## ğŸš€ Quick Start

### Prerequisites
- **Python 3.8+** 
- **Node.js 16+**
- **llama.cpp server** running at `http://127.0.0.1:1234` (or Ollama)

### Easy Setup (Recommended)
```bash
# Clone/navigate to the project directory
cd "Swift Neethi"

# Run the startup script (starts both backend and frontend)
./start.sh
```

This will:
- âœ… Check prerequisites
- ğŸ“¦ Install all dependencies
- ğŸš€ Start backend at `http://localhost:8000`
- ğŸ¨ Start frontend at `http://localhost:3000`
- ğŸ“š Provide API docs at `http://localhost:8000/docs`

### Manual Setup

#### Backend Setup
```bash
cd backend
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
cp .env.example .env
# Edit .env with your settings
python3 main.py
```

#### Frontend Setup
```bash
cd frontend
npm install
npm start
```

## ğŸ—ï¸ Architecture

```
Swift Neethi/
â”œâ”€â”€ ğŸ–¥ï¸  frontend/          # React TypeScript UI
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/    # Reusable components
â”‚   â”‚   â”œâ”€â”€ services/      # API integration
â”‚   â”‚   â”œâ”€â”€ types.ts       # TypeScript definitions
â”‚   â”‚   â””â”€â”€ App.tsx        # Main application
â”œâ”€â”€ ğŸ”§ backend/            # Python FastAPI server
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ agents/        # AI agent implementations
â”‚   â”‚   â”œâ”€â”€ providers/     # LLM provider interfaces
â”‚   â”‚   â”œâ”€â”€ routes/        # API endpoints
â”‚   â”‚   â”œâ”€â”€ models/        # Data models
â”‚   â”‚   â””â”€â”€ middleware/    # Request processing
â”‚   â””â”€â”€ main.py           # FastAPI application
â””â”€â”€ ğŸ“„ README.md          # This file
```

## ğŸ”§ Configuration

### Backend Configuration (.env)
```bash
# Provider URLs
LLAMACPP_BASE_URL=http://127.0.0.1:1234
OLLAMA_BASE_URL=http://localhost:11434

# Default Models
LLAMACPP_DEFAULT_MODEL=qwen/qwen3-4b
OLLAMA_DEFAULT_MODEL=llama2

# Server Settings
HOST=0.0.0.0
PORT=8000
DEBUG=True

# Agent Settings
MAX_CONTEXT_LENGTH=4096
DEFAULT_TEMPERATURE=0.7
```

### Frontend Configuration
The frontend automatically connects to the backend at `http://localhost:8000`. To change this, edit `frontend/src/services/api.ts`.

## ğŸ“¡ API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/chat` | POST | Send chat message |
| `/api/chat/stream` | POST | Stream chat response |
| `/api/models` | GET | List available models |
| `/api/models/providers` | GET | Check provider status |
| `/api/prompts` | GET/POST/PUT/DELETE | Manage system prompts |
| `/docs` | GET | Interactive API documentation |

## ğŸ¤– Supported AI Providers

### llama.cpp
- âœ… **Chat Completions**: `/v1/chat/completions`
- âœ… **Streaming**: Real-time response streaming
- âœ… **Model List**: `/v1/models`
- âœ… **Multi-model**: Support for multiple loaded models

### Ollama
- âœ… **Chat API**: Native Ollama chat interface
- âœ… **Streaming**: Real-time response streaming
- âœ… **Model Management**: List and switch models
- âœ… **Auto-discovery**: Automatic model detection

## ğŸ¯ Usage Examples

### Basic Chat
1. Open `http://localhost:3000`
2. Click "New Chat" to start
3. Type your message and press Enter
4. Watch the AI response stream in real-time

### System Prompts
Create custom AI personalities and contexts:
```json
{
  "name": "Code Helper",
  "content": "You are an expert programmer. Help with coding questions and provide clear examples.",
  "variables": ["language", "difficulty"]
}
```

### API Integration
```javascript
// Send a chat message
const response = await fetch('http://localhost:8000/api/chat', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    messages: [{ role: 'user', content: 'Hello!' }],
    provider: 'llamacpp',
    model: 'qwen/qwen3-4b'
  })
});
```

## ğŸ› ï¸ Development

### Testing the Backend
```bash
cd backend
python3 quick_test.py
```

### Building for Production
```bash
# Frontend
cd frontend
npm run build

# Backend
cd backend
pip install gunicorn
gunicorn main:app
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## ğŸ“ License

MIT License - see LICENSE file for details

## ğŸ†˜ Troubleshooting

### Common Issues

**Backend won't start:**
- Check Python version: `python3 --version`
- Verify virtual environment: `source venv/bin/activate`
- Check port 8000 availability

**Frontend won't connect:**
- Ensure backend is running at `http://localhost:8000`
- Check browser console for CORS errors
- Verify API URL in `frontend/src/services/api.ts`

**llama.cpp not working:**
- Verify server is running at `http://127.0.0.1:1234`
- Test endpoints: `curl http://127.0.0.1:1234/v1/models`
- Check model is loaded properly

### Getting Help
- ğŸ“š Check the API docs: `http://localhost:8000/docs`
- ğŸ› Review browser console for errors
- ğŸ” Check backend logs for detailed error messages

---

**Built with â¤ï¸ for local AI development**